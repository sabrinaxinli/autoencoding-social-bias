{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_save_path = \"./extracted/SBIC.v2.trn.csv\"\n",
    "dev_data_save_path = \"./extracted/SBIC.v2.dev.csv\"\n",
    "test_data_save_path = \"./extracted/SBIC.v2.tst.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while extracting the file: [Errno 2] No such file or directory: '/Users/sabrina/Computational Social Science/final-project/SBIC.v2.tgz'\n",
      "Error occurred while reading the file: [Errno 2] No such file or directory: '/Users/sabrina/Computational Social Science/final-project/extracted\\\\SBIC.v2.tst.csv'\n"
     ]
    }
   ],
   "source": [
    "from process_data import read_tgz_data\n",
    "\n",
    "train_data = read_tgz_data(train_data_save_path)\n",
    "dev_data = read_tgz_data(dev_data_save_path)\n",
    "test_data = read_tgz_data(test_data_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_context_str(category, target_group, target_stereotype):\n",
    "    category, target_group, target_stereotype = str(category), str(target_group), str(target_stereotype)\n",
    "\n",
    "    if target_stereotype.startswith(target_group):\n",
    "        return \" \".join((category, \":\", target_stereotype))\n",
    "    else:\n",
    "        return \" \".join((category, \":\", target_group, target_stereotype))\n",
    "\n",
    "def normalize_spacing(input):\n",
    "    cleaned_string = input.strip()\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    return cleaned_string\n",
    "\n",
    "def remove_html_entities(text):\n",
    "    return re.sub(r\"&#[0-9]+;\", \"\", text)\n",
    "\n",
    "def remove_rt_username(text):\n",
    "    return re.sub(r\"RT @\\w+\\s*:\", \"\", text)\n",
    "\n",
    "def remove_beginning_and_ending_tags(text):\n",
    "    text = re.sub(r\"^(?:@[A-Za-z0-9_]+ )+\", \"\", text)\n",
    "    text = re.sub(r\"(?: @[A-Za-z0-9_]+)+$\", \"\", text)\n",
    "    text = re.sub(r\"^(?:@[A-Za-z0-9_]+[^\\w\\s]? )+\", \"\", text)\n",
    "    text = re.sub(r\"^(?:\\.\\s*)?(?:@[A-Za-z0-9_]+ )+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_start_html(text):\n",
    "    return re.sub(r\"&gt;\", \"\", text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "                               \"\\U0001F300-\\U0001F5FF\"\n",
    "                               \"\\U0001F680-\\U0001F6FF\"\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return re.sub(emoji_pattern, \"\", text)\n",
    "\n",
    "def clean_post(post):\n",
    "    post = remove_html_entities(post)\n",
    "    post = remove_rt_username(post)\n",
    "    post = remove_beginning_and_ending_tags(post)\n",
    "    post = remove_emojis(post)\n",
    "    post = remove_start_html(post)\n",
    "    post = normalize_spacing(post)\n",
    "    post = re.sub(r\"@\", \"\", post)\n",
    "    post = re.sub(r\"#\", \"\", post)\n",
    "    return post\n",
    "\n",
    "def merge_same_posts(posts):\n",
    "    first_row = posts.iloc[0]\n",
    "    posts = posts.fillna(\"nan\")\n",
    "    implications = posts[\"targetStereotype\"].tolist()\n",
    "    targeted_groups = posts[\"targetMinority\"].tolist()\n",
    "    targeted_categories = posts[\"targetCategory\"].tolist()\n",
    "    \n",
    "    contexts = list(zip(targeted_categories, targeted_groups, implications))\n",
    "    nan_context = (\"nan\", \"nan\", \"nan\")\n",
    "    filtered_contexts = [context for context in contexts if context != nan_context]\n",
    "    contexts = [get_context_str(tcat, tgroup, implication) for (tcat, tgroup, implication) in filtered_contexts]\n",
    "    # print(f\"Contexts: {contexts}\")\n",
    "    first_row[\"context\"] = contexts\n",
    "\n",
    "    raw_post = first_row[\"post\"]\n",
    "    first_row[\"post\"] = clean_post(raw_post)\n",
    "    return first_row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\AppData\\Local\\Temp\\ipykernel_16520\\602313721.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_train = train_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
      "C:\\Users\\sabri\\AppData\\Local\\Temp\\ipykernel_16520\\602313721.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_dev = dev_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
      "C:\\Users\\sabri\\AppData\\Local\\Temp\\ipykernel_16520\\602313721.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_test = test_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n"
     ]
    }
   ],
   "source": [
    "merged_train = train_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
    "merged_dev = dev_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
    "merged_test = test_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged\n",
    "merged_train_savepath = \"./data/merged/merged_train.csv\"\n",
    "merged_dev_savepath = \"./data/merged/merged_dev.csv\"\n",
    "merged_test_savepath = \"./data/merged/merged_test.csv\"\n",
    "\n",
    "merged_train.to_csv(merged_train_savepath, index=False)\n",
    "merged_dev.to_csv(merged_dev_savepath, index=False)\n",
    "merged_test.to_csv(merged_test_savepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_len(x):\n",
    "    return len(x) if isinstance(x, list) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with impl\n",
    "merged_train_impl_savepath = \"./data/with_impl/merged_train.csv\"\n",
    "merged_dev_impl_savepath = \"./data/with_impl/merged_dev.csv\"\n",
    "merged_test_impl_savepath = \"./data/with_impl/merged_test.csv\"\n",
    "\n",
    "merged_train_impl = merged_train[merged_train[\"context\"].map(safe_len) > 0]\n",
    "merged_dev_impl = merged_dev[merged_dev[\"context\"].map(safe_len) > 0]\n",
    "merged_test_impl = merged_test[merged_test[\"context\"].map(safe_len) > 0]\n",
    "\n",
    "merged_train_impl.to_csv(merged_train_impl_savepath, index=False)\n",
    "merged_dev_impl.to_csv(merged_dev_impl_savepath, index=False)\n",
    "merged_test_impl.to_csv(merged_test_impl_savepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 1010, 2088,  999,  102,    0,    0],\n",
      "        [ 101, 2182, 2003, 1037, 3231, 5164,  999,  102]]), 'token_type_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [2, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Testing to see tokenizer outputs\n",
    "import torch\n",
    "from transformers import FunnelTokenizer\n",
    "\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "\n",
    "# Tests\n",
    "texts = [\"Hello, world!\", \"Here is a test string!\"]\n",
    "\n",
    "tokenized_output = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "model = FunnelModel.from_pretrained(\"funnel-transformer/small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "MAX_TOK_LEN = 70\n",
    "# Tokenize post helper function\n",
    "def tokenize_post(row):\n",
    "    context_str = \" <sep> \".join(row[\"context\"]) + \" <sep>\"\n",
    "    post_str = \"<cls> \" + row[\"post\"]\n",
    "    input_str = post_str + context_str\n",
    "    \n",
    "    post_data = row[\"post\"]\n",
    "    # print(f\"Input str: {input_str}\")\n",
    "    tokenized_output = tokenizer(input_str, padding=\"max_length\", add_special_tokens=False, truncation=True, max_length = MAX_TOK_LEN, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_output[\"input_ids\"].numpy()\n",
    "    attention_mask = tokenized_output[\"attention_mask\"].numpy()\n",
    "    token_type_ids = tokenized_output[\"token_type_ids\"].numpy()\n",
    "    \n",
    "    tokenized_post = tokenizer(post_data, padding=\"max_length\", max_length = MAX_TOK_LEN, truncation=True, return_tensors=\"pt\")\n",
    "    target_labels = tokenized_post[\"input_ids\"].numpy()\n",
    "    # token_type_ids_np = np.zeros_like(tokenized_output[\"token_type_ids\"])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"target_labels\": target_labels\n",
    "    }\n",
    "\n",
    "def t5_tokenize_post_reconstruction_prompt(row):\n",
    "    summary = row[\"summary\"]\n",
    "    context = row[\"context\"]\n",
    "    input_prompt = f\"Based on this summary: <{summary}>, and the implications of the post <{context}>, reconstruct the post.\"\n",
    "    tokenized_output = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    return tokenized_output.numpy()\n",
    "\n",
    "# Contexts is a list of strings\n",
    "# def tokenize_context(contexts):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "#     for context in contexts:\n",
    "#         tokenized_output = tokenizer(context, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         context_tokens.append((tokenized_output[\"input_ids\"], tokenized_output[\"token_type_ids\"], tokenized_output[\"attention_mask\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whoTarget', 'intentYN', 'sexYN', 'sexReason', 'offensiveYN', 'annotatorGender', 'annotatorMinority', 'sexPhrase', 'speakerMinorityYN', 'WorkerId', 'HITId', 'annotatorPolitics', 'annotatorRace', 'annotatorAge', 'post', 'targetMinority', 'targetCategory', 'targetStereotype', 'dataSource', 'context', 'input_ids', 'token_type_ids', 'attention_mask', 'target_labels']\n"
     ]
    }
   ],
   "source": [
    "# ## FOR FUNNEL TRANSFORMER ###\n",
    "\n",
    "# # Tokenize and add to df\n",
    "# train_tokens = merged_train_impl.apply(tokenize_post, axis = 1)\n",
    "# train_tokens_df = pd.DataFrame(train_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "# train_tokenized_df = pd.concat([merged_train.reset_index(drop=True), train_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# print(train_tokenized_df.columns.tolist())\n",
    "\n",
    "# # Tokenize and add to df\n",
    "# dev_tokens = merged_dev_impl.apply(tokenize_post, axis = 1)\n",
    "# dev_tokens_df = pd.DataFrame(dev_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "# dev_tokenized_df = pd.concat([merged_dev.reset_index(drop=True), dev_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "# # Tokenize and add to df\n",
    "# test_tokens = merged_test_impl.apply(tokenize_post, axis = 1)\n",
    "# test_tokens_df = pd.DataFrame(test_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "# test_tokenized_df = pd.concat([merged_test.reset_index(drop=True), test_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "# train_tokenized_df.to_json(\"./data/tokenized/train_tokenized.json\", orient = \"records\", lines = True)\n",
    "# dev_tokenized_df.to_json(\"./data/tokenized/dev_tokenized.json\", orient = \"records\", lines = True)\n",
    "# test_tokenized_df.to_json(\"./data/tokenized/test_tokenized.json\", orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whoTarget', 'intentYN', 'sexYN', 'sexReason', 'offensiveYN', 'annotatorGender', 'annotatorMinority', 'sexPhrase', 'speakerMinorityYN', 'WorkerId', 'HITId', 'annotatorPolitics', 'annotatorRace', 'annotatorAge', 'post', 'targetMinority', 'targetCategory', 'targetStereotype', 'dataSource', 'context', 'input_ids', 'token_type_ids', 'attention_mask', 'target_labels']\n"
     ]
    }
   ],
   "source": [
    "## FOR FUNNEL TRANSFORMER ### ### FOR WITH_IMPL\n",
    "\n",
    "# Tokenize and add to df\n",
    "train_tokens = merged_train_impl.apply(tokenize_post, axis = 1)\n",
    "train_tokens_df = pd.DataFrame(train_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "train_tokenized_df = pd.concat([merged_train_impl.reset_index(drop=True), train_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(train_tokenized_df.columns.tolist())\n",
    "\n",
    "# Tokenize and add to df\n",
    "dev_tokens = merged_dev_impl.apply(tokenize_post, axis = 1)\n",
    "dev_tokens_df = pd.DataFrame(dev_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "dev_tokenized_df = pd.concat([merged_dev_impl.reset_index(drop=True), dev_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "# Tokenize and add to df\n",
    "test_tokens = merged_test_impl.apply(tokenize_post, axis = 1)\n",
    "test_tokens_df = pd.DataFrame(test_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "test_tokenized_df = pd.concat([merged_test_impl.reset_index(drop=True), test_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "train_tokenized_df.to_json(\"./data/tokenized/train_tokenized.json\", orient = \"records\", lines = True)\n",
    "dev_tokenized_df.to_json(\"./data/tokenized/dev_tokenized.json\", orient = \"records\", lines = True)\n",
    "test_tokenized_df.to_json(\"./data/tokenized/test_tokenized.json\", orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_string_to_list(string):\n",
    "\treturn ast.literal_eval(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading data back in rather than rerunning above\n",
    "merged_train_savepath = \"./data/merged/merged_train.csv\"\n",
    "merged_dev_savepath = \"./data/merged/merged_dev.csv\"\n",
    "merged_test_savepath = \"./data/merged/merged_test.csv\"\n",
    "\n",
    "merged_train = pd.read_csv(merged_train_savepath)\n",
    "merged_dev = pd.read_csv(merged_dev_savepath)\n",
    "merged_test = pd.read_csv(merged_test_savepath)\n",
    "\n",
    "merged_train_impl_savepath = \"./data/with_impl/merged_train.csv\"\n",
    "merged_dev_impl_savepath = \"./data/with_impl/merged_dev.csv\"\n",
    "merged_test_impl_savepath = \"./data/with_impl/merged_test.csv\"\n",
    "\n",
    "merged_train_impl = pd.read_csv(merged_train_impl_savepath)\n",
    "merged_dev_impl = pd.read_csv(merged_dev_impl_savepath)\n",
    "merged_test_impl = pd.read_csv(merged_test_impl_savepath)\n",
    "\n",
    "train_tokenized_df = pd.read_json(\"./data/tokenized/train_tokenized.json\", orient = \"records\", lines = True)\n",
    "dev_tokenized_df = pd.read_json(\"./data/tokenized/dev_tokenized.json\", orient = \"records\", lines = True)\n",
    "test_tokenized_df = pd.read_json(\"./data/tokenized/test_tokenized.json\", orient = \"records\", lines = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BiasDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"test\")\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        # print(f\"Input ids: {input_ids}\")\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        token_type_ids = self.dataset.loc[idx, \"token_type_ids\"]\n",
    "        target_labels = self.dataset.loc[idx, \"target_labels\"]\n",
    "        \n",
    "        # print(type(input_ids))\n",
    "        # print(type(attention_mask))\n",
    "        # print(type(token_type_ids))\n",
    "        # print(type(target_labels))\n",
    "        # print(\"test2\")\n",
    "        # Create input tensors\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long).squeeze(dim=0),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long).squeeze(dim=0),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long).squeeze(dim=0),\n",
    "            \"target_labels\": torch.tensor(target_labels, dtype=torch.long).squeeze(dim=0)\n",
    "        }\n",
    "\n",
    "        # print(\"test3\")\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    BiasDataset(train_tokenized_df),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    BiasDataset(test_tokenized_df.iloc[:50]),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RegenerativeTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(RegenerativeTransformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.embedding = nn.Embedding(30522, 768)\n",
    "        self.output = nn.Linear(768, 30522)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        memory = self.encoder(src, attention_mask=src_mask)[0]  # Ensure output matches expected format\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrina/Computational Social Science/final-project/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# MAX_LENGTH = 50\n",
    "# def train_loop(dataloader, model, loss_fn, encoder_optimizer, decoder_optimizer, start_token_id):\n",
    "#     train_loss = 0\n",
    "#     # set the model to training model\n",
    "#     model.train()\n",
    "#     iter_count = 0\n",
    "#     # for batch in dataloader:\n",
    "#     for batch in tqdm.tqdm(dataloader):\n",
    "#         encoder_optimizer.zero_grad()\n",
    "#         decoder_optimizer.zero_grad()\n",
    "        \n",
    "#         # previous tokens\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "#         target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "        \n",
    "#         target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "#         if target_pad > 0:\n",
    "#              target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "        \n",
    "#         # print(f\"Input ids: {input_ids.shape}\")\n",
    "#         # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "#         encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "#         encoder_state = encoder_outputs.last_hidden_state\n",
    "#         # Decoder part initiated\n",
    "        \n",
    "#         start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "#         start_token_embed = model.embedding(start_token_id)\n",
    "#         decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "        \n",
    "#         # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)\n",
    "#         outputs = []\n",
    "\n",
    "#         for i in range(MAX_LENGTH):\n",
    "#             # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "#             # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "#             decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "            \n",
    "#             logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "#             outputs.append(logits)\n",
    "            \n",
    "#             decoder_input = target_labels[:, i]\n",
    "#             decoder_input = model.embedding(decoder_input).unsqueeze(dim = 1)\n",
    "            \n",
    "\n",
    "#             # _, topi = logits.topk(1)\n",
    "#             # decoder_input = topi.squeeze().detach()\n",
    "#             # decoder_input = model.embedding(decoder_input).repeat(input_ids.size(0), 1, 1)\n",
    "\n",
    "#         outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "#         # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "#         # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "        \n",
    "#         # preds = torch.argmax(outputs, dim = 2)\n",
    "#         mask = target_labels != 0\n",
    "#         # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#         # # print(f\"Loss shape: {loss.shape}\")\n",
    "#         # loss = loss.view(target_labels.shape)\n",
    "#         # masked_loss = loss * mask.float()\n",
    "\n",
    "#         # loss_sum = masked_loss.sum()\n",
    "#         # num_valid_tokens = mask.sum()\n",
    "#         # loss = loss_sum / num_valid_tokens.float()\n",
    "\n",
    "#         loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#         mask = target_labels != 0\n",
    "#         masked_loss = loss * mask.view(-1).float()\n",
    "#         loss = masked_loss.sum() / mask.sum()\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "\n",
    "#         if iter_count % 5 == 0:\n",
    "#             print(f\"Loss: {loss.item()}\")\n",
    "#         loss.backward()\n",
    "\n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "\n",
    "#         iter_count += 1\n",
    "    \n",
    "#     return train_loss / len(dataloader)\n",
    "\n",
    "# def test_loop(dataloader, model, loss_fn, tokenizer, start_token_id):\n",
    "\n",
    "#     all_sentences = []\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm.tqdm(dataloader):\n",
    "#             # previous tokens\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "#             target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "            \n",
    "#             target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "#             if target_pad > 0:\n",
    "#                 target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "            \n",
    "#             # print(f\"Input ids: {input_ids.shape}\")\n",
    "#             # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "#             encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "#             encoder_state = encoder_outputs.last_hidden_state\n",
    "            \n",
    "#             start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "#             start_token_embed = model.embedding(start_token_id)\n",
    "#             decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "            \n",
    "#             # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)  # Start token\n",
    "#             outputs = []\n",
    "\n",
    "#             for i in range(MAX_LENGTH):\n",
    "#                 print(f\"Decoder input: {decoder_input.shape}\")\n",
    "#                 # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "#                 decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "                \n",
    "#                 logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "#                 outputs.append(logits)\n",
    "\n",
    "#                 _, topi = logits.topk(1)\n",
    "#                 decoder_input = topi.squeeze().detach()\n",
    "#                 decoder_input = model.embedding(decoder_input).unsqueeze(dim = 0).unsqueeze(dim = 0)\n",
    "\n",
    "#             outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "#             # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "#             # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "            \n",
    "#             preds = torch.argmax(outputs, dim = 2)\n",
    "#             # mask = target_labels != 0\n",
    "#             # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#             # loss = loss.view(target_labels.shape)\n",
    "#             # masked_loss = loss * mask.float()\n",
    "\n",
    "#             loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#             mask = target_labels != 0\n",
    "#             masked_loss = loss * mask.view(-1).float()\n",
    "#             loss = masked_loss.sum() / mask.sum()\n",
    "\n",
    "#             loss_sum = masked_loss.sum()\n",
    "#             num_valid_tokens = mask.sum()\n",
    "#             loss = loss_sum / num_valid_tokens.float()\n",
    "#             print(f\"Loss: {loss.item()}\")\n",
    "#             val_loss += loss.item()\n",
    "            \n",
    "#             decoded_sentences = [tokenizer.decode(pred, skip_special_tokens=True) for pred in preds]\n",
    "#             all_sentences.extend(decoded_sentences)\n",
    "\n",
    "#     return val_loss / len(dataloader), all_sentences\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# import torch.nn.functional as F\n",
    "# import random\n",
    "\n",
    "# MAX_LENGTH = 50\n",
    "# def train_loop(dataloader, model, loss_fn, encoder_optimizer, decoder_optimizer, start_token_id):\n",
    "#     train_loss = 0\n",
    "#     # set the model to training model\n",
    "#     model.train()\n",
    "#     iter_count = 0\n",
    "#     # for batch in dataloader:\n",
    "#     for batch in tqdm.tqdm(dataloader):\n",
    "#         encoder_optimizer.zero_grad()\n",
    "#         decoder_optimizer.zero_grad()\n",
    "        \n",
    "#         # previous tokens\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "#         target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "        \n",
    "#         target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "#         if target_pad > 0:\n",
    "#              target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "        \n",
    "#         # print(f\"Input ids: {input_ids.shape}\")\n",
    "#         # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "#         encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "#         encoder_state = encoder_outputs.last_hidden_state\n",
    "#         # Decoder part initiated\n",
    "        \n",
    "#         start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "#         start_token_embed = model.embedding(start_token_id)\n",
    "#         decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "        \n",
    "#         # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)\n",
    "#         outputs = []\n",
    "\n",
    "#         teacher_forcing_ratio = 0.5\n",
    "\n",
    "#         for i in range(MAX_LENGTH):\n",
    "#             # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "#             # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "#             decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "            \n",
    "#             logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "#             outputs.append(logits)\n",
    "\n",
    "#             _, topi = logits.topk(1)\n",
    "            \n",
    "#             use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "#             if use_teacher_forcing and i < target_labels.size(1) - 1:\n",
    "#                 next_token = target_labels[i + 1]\n",
    "#             else:\n",
    "#                 next_token = topi.squeeze().detach()\n",
    "\n",
    "#             print(\"next tok\")\n",
    "#             print(next_token.shape)\n",
    "#             print(f\"I: {i}\")\n",
    "#             next_input = model.embedding(torch.tensor([next_token])).unsqueeze(dim = 1)\n",
    "#             decoder_input = torch.cat((decoder_input, next_input), dim=1)\n",
    "\n",
    "#             # _, topi = logits.topk(1)\n",
    "#             # decoder_input = topi.squeeze().detach()\n",
    "#             # decoder_input = model.embedding(decoder_input).repeat(input_ids.size(0), 1, 1)\n",
    "\n",
    "#         outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "#         # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "#         # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "        \n",
    "#         # preds = torch.argmax(outputs, dim = 2)\n",
    "#         mask = target_labels != 0\n",
    "#         # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#         # # print(f\"Loss shape: {loss.shape}\")\n",
    "#         # loss = loss.view(target_labels.shape)\n",
    "#         # masked_loss = loss * mask.float()\n",
    "\n",
    "#         # loss_sum = masked_loss.sum()\n",
    "#         # num_valid_tokens = mask.sum()\n",
    "#         # loss = loss_sum / num_valid_tokens.float()\n",
    "\n",
    "#         loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#         mask = target_labels != 0\n",
    "#         masked_loss = loss * mask.view(-1).float()\n",
    "#         loss = masked_loss.sum() / mask.sum()\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "\n",
    "#         if iter_count % 5 == 0:\n",
    "#             print(f\"Loss: {loss.item()}\")\n",
    "#         loss.backward()\n",
    "\n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "\n",
    "#         iter_count += 1\n",
    "    \n",
    "#     return train_loss / len(dataloader)\n",
    "\n",
    "# def test_loop(dataloader, model, loss_fn, tokenizer, start_token_id):\n",
    "\n",
    "#     all_sentences = []\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm.tqdm(dataloader):\n",
    "#             # previous tokens\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "#             target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "            \n",
    "#             target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "#             if target_pad > 0:\n",
    "#                 target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "            \n",
    "#             # print(f\"Input ids: {input_ids.shape}\")\n",
    "#             # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "#             encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "#             encoder_state = encoder_outputs.last_hidden_state\n",
    "            \n",
    "#             start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "#             start_token_embed = model.embedding(start_token_id)\n",
    "#             decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "            \n",
    "#             # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)  # Start token\n",
    "#             outputs = []\n",
    "\n",
    "#             for i in range(MAX_LENGTH):\n",
    "#                 print(f\"Decoder input: {decoder_input.shape}\")\n",
    "#                 # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "#                 decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "                \n",
    "#                 logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "#                 outputs.append(logits)\n",
    "\n",
    "#                 _, topi = logits.topk(1)\n",
    "#                 decoder_input = topi.squeeze().detach()\n",
    "#                 decoder_input = model.embedding(decoder_input).unsqueeze(dim = 0).unsqueeze(dim = 0)\n",
    "\n",
    "#             outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "#             # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "#             # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "            \n",
    "#             preds = torch.argmax(outputs, dim = 2)\n",
    "#             # mask = target_labels != 0\n",
    "#             # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#             # loss = loss.view(target_labels.shape)\n",
    "#             # masked_loss = loss * mask.float()\n",
    "\n",
    "#             loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#             mask = target_labels != 0\n",
    "#             masked_loss = loss * mask.view(-1).float()\n",
    "#             loss = masked_loss.sum() / mask.sum()\n",
    "\n",
    "#             loss_sum = masked_loss.sum()\n",
    "#             num_valid_tokens = mask.sum()\n",
    "#             loss = loss_sum / num_valid_tokens.float()\n",
    "#             print(f\"Loss: {loss.item()}\")\n",
    "#             val_loss += loss.item()\n",
    "            \n",
    "#             decoded_sentences = [tokenizer.decode(pred, skip_special_tokens=True) for pred in preds]\n",
    "#             all_sentences.extend(decoded_sentences)\n",
    "\n",
    "#     return val_loss / len(dataloader), all_sentences\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "MAX_LENGTH = 50\n",
    "def train_loop(dataloader, model, loss_fn, encoder_optimizer, decoder_optimizer, start_token_id):\n",
    "    train_loss = 0\n",
    "    # set the model to training model\n",
    "    model.train()\n",
    "    iter_count = 0\n",
    "    # for batch in dataloader:\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        # previous tokens\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "        \n",
    "        target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "        if target_pad > 0:\n",
    "             target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "        \n",
    "        # print(f\"Input ids: {input_ids.shape}\")\n",
    "        # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "        encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        encoder_state = encoder_outputs.last_hidden_state\n",
    "        # Decoder part initiated\n",
    "        \n",
    "        start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "        start_token_embed = model.embedding(start_token_id)\n",
    "        decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "        \n",
    "        # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)\n",
    "        outputs = []\n",
    "\n",
    "        teacher_forcing_ratio = 1.0\n",
    "        for i in range(MAX_LENGTH):\n",
    "            # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "            # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "            decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "            \n",
    "            logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "            outputs.append(logits)\n",
    "            _, topi = logits.topk(1)\n",
    "            \n",
    "            \n",
    "            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "            if use_teacher_forcing and i < target_labels.size(1) - 1:\n",
    "                next_token = target_labels[:, i + 1]\n",
    "            else:\n",
    "                next_token = topi.squeeze().detach()\n",
    "\n",
    "            decoder_input = model.embedding(next_token).unsqueeze(dim = 1)\n",
    "\n",
    "            # _, topi = logits.topk(1)\n",
    "            # decoder_input = topi.squeeze().detach()\n",
    "            # decoder_input = model.embedding(decoder_input).repeat(input_ids.size(0), 1, 1)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "        # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "        # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "        \n",
    "        # preds = torch.argmax(outputs, dim = 2)\n",
    "        mask = target_labels != 0\n",
    "        # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "        # # print(f\"Loss shape: {loss.shape}\")\n",
    "        # loss = loss.view(target_labels.shape)\n",
    "        # masked_loss = loss * mask.float()\n",
    "\n",
    "        # loss_sum = masked_loss.sum()\n",
    "        # num_valid_tokens = mask.sum()\n",
    "        # loss = loss_sum / num_valid_tokens.float()\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "        mask = target_labels != 0\n",
    "        masked_loss = loss * mask.view(-1).float()\n",
    "        loss = masked_loss.sum() / mask.sum()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "        if iter_count % 5 == 0:\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        iter_count += 1\n",
    "    \n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, tokenizer, start_token_id):\n",
    "\n",
    "    all_sentences = []\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            # previous tokens\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "            \n",
    "            target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "            if target_pad > 0:\n",
    "                target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "            \n",
    "            # print(f\"Input ids: {input_ids.shape}\")\n",
    "            # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "            encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "            encoder_state = encoder_outputs.last_hidden_state\n",
    "            \n",
    "            start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "            start_token_embed = model.embedding(start_token_id)\n",
    "            decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "            \n",
    "            # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)  # Start token\n",
    "            outputs = []\n",
    "\n",
    "            for i in range(MAX_LENGTH):\n",
    "                print(f\"Decoder input: {decoder_input.shape}\")\n",
    "                # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "                decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "                \n",
    "                logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "                outputs.append(logits)\n",
    "\n",
    "                _, topi = logits.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = model.embedding(decoder_input).unsqueeze(dim = 0).unsqueeze(dim = 0)\n",
    "\n",
    "            outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "            # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "            # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim = 2)\n",
    "            # mask = target_labels != 0\n",
    "            # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "            # loss = loss.view(target_labels.shape)\n",
    "            # masked_loss = loss * mask.float()\n",
    "\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "            mask = target_labels != 0\n",
    "            masked_loss = loss * mask.view(-1).float()\n",
    "            loss = masked_loss.sum() / mask.sum()\n",
    "\n",
    "            loss_sum = masked_loss.sum()\n",
    "            num_valid_tokens = mask.sum()\n",
    "            loss = loss_sum / num_valid_tokens.float()\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            decoded_sentences = [tokenizer.decode(pred, skip_special_tokens=True) for pred in preds]\n",
    "            all_sentences.extend(decoded_sentences)\n",
    "\n",
    "    return val_loss / len(dataloader), all_sentences\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer\n",
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "import torch.optim as optim\n",
    "\n",
    "encoder = FunnelModel.from_pretrained(\"funnel-transformer/small\")\n",
    "encoder_tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "print(encoder_tokenizer.vocab_size)\n",
    "decoder_layer = TransformerDecoderLayer(d_model=768, nhead=8, batch_first = True)\n",
    "decoder = TransformerDecoder(decoder_layer, num_layers = 6)\n",
    "\n",
    "regenerative_model = RegenerativeTransformer(encoder, decoder).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(regenerative_model.encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(regenerative_model.decoder.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "0\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "print(encoder_tokenizer.pad_token)\n",
    "print(encoder_tokenizer.convert_tokens_to_ids(encoder_tokenizer.pad_token))\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = encoder_tokenizer.convert_tokens_to_ids(encoder_tokenizer.bos_token)\n",
    "# train_loop(train_dataloader, regenerative_model, loss_fn, encoder_optimizer, decoder_optimizer, bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/380 [00:00<?, ?it/s]d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.414011001586914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 5/380 [03:35<2:34:39, 24.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.078265190124512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/380 [14:02<7:30:05, 72.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.121973037719727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 15/380 [1:23:02<63:19:40, 624.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.868656158447266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 16/380 [1:23:12<44:27:00, 439.62s/it]"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "EPOCHS = 5\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "\ttrain_loss = train_loop(train_dataloader, regenerative_model, loss_fn, encoder_optimizer, decoder_optimizer, bos_token_id)\n",
    "\tprint(f\"Epoch: {train_loss}\")\n",
    "\ttrain_losses.append(train_loss)\n",
    "\tif epoch % 1 == 0:\n",
    "\t\tval_loss, val_sentences = test_loop(test_dataloader, regenerative_model, loss_fn, encoder_tokenizer, bos_token_id)\n",
    "\t\tval_losses.append(val_loss)\n",
    "\t\tcheckpoint = {\n",
    "\t\t\t\"model\": regenerative_model.state_dict(),\n",
    "\t\t\t\"encoder_optimizer\": encoder_optimizer.state_dict(),\n",
    "\t\t\t\"decoder_optimizer\": decoder_optimizer.state_dict(),\n",
    "\t\t\t\"train_losses\": train_losses,\n",
    "\t\t\t\"val_losses\": val_losses,\n",
    "\t\t\t\"val_sentences\": val_sentences,\n",
    "\t\t\t\"epoch\": epoch\n",
    "\t\t}\n",
    "\t\ttorch.save(checkpoint, f\"./checkpoints/checkpoint_{epoch}.pt\")\n",
    "\n",
    "\n",
    "print(f\"Train Losses: {train_losses}\")\n",
    "print(f\"Val Losses: {val_losses}\")\n",
    "\n",
    "for sent in val_sentences:\n",
    "\tprint(sent)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrina/Computational Social Science/final-project/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.1376, -0.3090,  0.6842,  ..., -0.4041,  0.8737, -0.7017],\n",
      "         [-0.1957, -0.2551,  0.0395,  ...,  0.2188,  0.1387,  0.1632],\n",
      "         [-0.2267,  0.3701,  0.0961,  ...,  0.0500, -0.0244, -0.1002],\n",
      "         ...,\n",
      "         [-0.5314,  0.7220,  0.2493,  ...,  0.0725,  0.0889, -0.0084],\n",
      "         [-0.1779,  0.1923,  0.5646,  ...,  0.4919,  0.7747, -1.1734],\n",
      "         [-0.1268,  0.0407,  0.0438,  ...,  0.6065,  1.0749, -1.3630]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "model = FunnelModel.from_pretrained(\"funnel-transformer/small\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
