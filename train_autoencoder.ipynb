{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_save_path = \"/Users/sabrina/Computational Social Science/final-project/extracted/SBIC.v2.trn.csv\"\n",
    "dev_data_save_path = \"/Users/sabrina/Computational Social Science/final-project/extracted/SBIC.v2.dev.csv\"\n",
    "test_data_save_path = \"/Users/sabrina/Computational Social Science/final-project/extracted/SBIC.v2.tst.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_data import read_tgz_data\n",
    "\n",
    "train_data = read_tgz_data(train_data_save_path)\n",
    "dev_data = read_tgz_data(dev_data_save_path)\n",
    "test_data = read_tgz_data(test_data_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_context_str(category, target_group, target_stereotype):\n",
    "    category, target_group, target_stereotype = str(category), str(target_group), str(target_stereotype)\n",
    "\n",
    "    if target_stereotype.startswith(target_group):\n",
    "        return \" \".join((category, \":\", target_stereotype))\n",
    "    else:\n",
    "        return \" \".join((category, \":\", target_group, target_stereotype))\n",
    "\n",
    "def normalize_spacing(input):\n",
    "    cleaned_string = input.strip()\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    return cleaned_string\n",
    "\n",
    "def remove_html_entities(text):\n",
    "    return re.sub(r\"&#[0-9]+;\", \"\", text)\n",
    "\n",
    "def remove_rt_username(text):\n",
    "    return re.sub(r\"RT @\\w+\\s*:\", \"\", text)\n",
    "\n",
    "def remove_beginning_and_ending_tags(text):\n",
    "    text = re.sub(r\"^(?:@[A-Za-z0-9_]+ )+\", \"\", text)\n",
    "    text = re.sub(r\"(?: @[A-Za-z0-9_]+)+$\", \"\", text)\n",
    "    text = re.sub(r\"^(?:@[A-Za-z0-9_]+[^\\w\\s]? )+\", \"\", text)\n",
    "    text = re.sub(r\"^(?:\\.\\s*)?(?:@[A-Za-z0-9_]+ )+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_start_html(text):\n",
    "    return re.sub(r\"&gt;\", \"\", text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "                               \"\\U0001F300-\\U0001F5FF\"\n",
    "                               \"\\U0001F680-\\U0001F6FF\"\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return re.sub(emoji_pattern, \"\", text)\n",
    "\n",
    "def clean_post(post):\n",
    "    post = remove_html_entities(post)\n",
    "    post = remove_rt_username(post)\n",
    "    post = remove_beginning_and_ending_tags(post)\n",
    "    post = remove_emojis(post)\n",
    "    post = remove_start_html(post)\n",
    "    post = normalize_spacing(post)\n",
    "    post = re.sub(r\"@\", \"\", post)\n",
    "    post = re.sub(r\"#\", \"\", post)\n",
    "    return post\n",
    "\n",
    "def merge_same_posts(posts):\n",
    "    first_row = posts.iloc[0]\n",
    "    posts = posts.fillna(\"nan\")\n",
    "    implications = posts[\"targetStereotype\"].tolist()\n",
    "    targeted_groups = posts[\"targetMinority\"].tolist()\n",
    "    targeted_categories = posts[\"targetCategory\"].tolist()\n",
    "    \n",
    "    contexts = list(zip(targeted_categories, targeted_groups, implications))\n",
    "    nan_context = (\"nan\", \"nan\", \"nan\")\n",
    "    filtered_contexts = [context for context in contexts if context != nan_context]\n",
    "    contexts = [get_context_str(tcat, tgroup, implication) for (tcat, tgroup, implication) in filtered_contexts]\n",
    "    # print(f\"Contexts: {contexts}\")\n",
    "    first_row[\"context\"] = contexts\n",
    "\n",
    "    raw_post = first_row[\"post\"]\n",
    "    first_row[\"post\"] = clean_post(raw_post)\n",
    "    return first_row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/g65_s8792fq1btbn7p4v2_p80000gn/T/ipykernel_80132/602313721.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_train = train_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
      "/var/folders/mt/g65_s8792fq1btbn7p4v2_p80000gn/T/ipykernel_80132/602313721.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_dev = dev_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
      "/var/folders/mt/g65_s8792fq1btbn7p4v2_p80000gn/T/ipykernel_80132/602313721.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_test = test_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n"
     ]
    }
   ],
   "source": [
    "merged_train = train_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
    "merged_dev = dev_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
    "merged_test = test_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged\n",
    "merged_train_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/merged/merged_train.csv\"\n",
    "merged_dev_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/merged/merged_dev.csv\"\n",
    "merged_test_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/merged/merged_test.csv\"\n",
    "\n",
    "merged_train.to_csv(merged_train_savepath, index=False)\n",
    "merged_dev.to_csv(merged_dev_savepath, index=False)\n",
    "merged_test.to_csv(merged_test_savepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with impl\n",
    "merged_train_impl_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/with_impl/merged_train.csv\"\n",
    "merged_dev_impl_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/with_impl/merged_dev.csv\"\n",
    "merged_test_impl_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/with_impl/merged_test.csv\"\n",
    "\n",
    "merged_train_impl = merged_train[merged_train[\"context\"].map(len) > 0]\n",
    "merged_dev_impl = merged_dev[merged_dev[\"context\"].map(len) > 0]\n",
    "merged_test_impl = merged_test[merged_test[\"context\"].map(len) > 0]\n",
    "\n",
    "merged_train_impl.to_csv(merged_train_impl_savepath, index=False)\n",
    "merged_dev_impl.to_csv(merged_dev_impl_savepath, index=False)\n",
    "merged_test_impl.to_csv(merged_test_impl_savepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrina/Computational Social Science/final-project/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 1010, 2088,  999,  102,    0,    0],\n",
      "        [ 101, 2182, 2003, 1037, 3231, 5164,  999,  102]]), 'token_type_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [2, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Testing to see tokenizer outputs\n",
    "import torch\n",
    "from transformers import FunnelTokenizer\n",
    "\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "\n",
    "# Tests\n",
    "texts = [\"Hello, world!\", \"Here is a test string!\"]\n",
    "\n",
    "tokenized_output = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "model = FunnelModel.from_pretrained(\"funnel-transformer/small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrina/Computational Social Science/final-project/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "\n",
    "# Tokenize post helper function\n",
    "def tokenize_post(row):\n",
    "    context_str = \" <sep> \".join(row[\"context\"]) + \" <sep>\"\n",
    "    post_str = \"<cls> \" + row[\"post\"]\n",
    "    input_str = post_str + context_str\n",
    "    \n",
    "    post_data = row[\"post\"]\n",
    "    # print(f\"Input str: {input_str}\")\n",
    "    tokenized_output = tokenizer(input_str, padding=True, add_special_tokens=False, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_output[\"input_ids\"].numpy()\n",
    "    attention_mask = tokenized_output[\"attention_mask\"].numpy()\n",
    "    token_type_ids = tokenized_output[\"token_type_ids\"].numpy()\n",
    "    \n",
    "    tokenized_post = tokenizer(post_data, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    target_labels = tokenized_post[\"input_ids\"].numpy()\n",
    "    # token_type_ids_np = np.zeros_like(tokenized_output[\"token_type_ids\"])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"target_labels\": target_labels\n",
    "    }\n",
    "\n",
    "def t5_tokenize_post_reconstruction_prompt(row):\n",
    "    summary = row[\"summary\"]\n",
    "    context = row[\"context\"]\n",
    "    input_prompt = f\"Based on this summary: <{summary}>, and the implications of the post <{context}>, reconstruct the post.\"\n",
    "    tokenized_output = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    return tokenized_output.numpy()\n",
    "\n",
    "# Contexts is a list of strings\n",
    "# def tokenize_context(contexts):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "#     for context in contexts:\n",
    "#         tokenized_output = tokenizer(context, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         context_tokens.append((tokenized_output[\"input_ids\"], tokenized_output[\"token_type_ids\"], tokenized_output[\"attention_mask\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whoTarget', 'intentYN', 'sexYN', 'sexReason', 'offensiveYN', 'annotatorGender', 'annotatorMinority', 'sexPhrase', 'speakerMinorityYN', 'WorkerId', 'HITId', 'annotatorPolitics', 'annotatorRace', 'annotatorAge', 'post', 'targetMinority', 'targetCategory', 'targetStereotype', 'dataSource', 'context', 'input_ids', 'token_type_ids', 'attention_mask', 'target_labels']\n"
     ]
    }
   ],
   "source": [
    "## FOR FUNNEL TRANSFORMER ###\n",
    "\n",
    "# Tokenize and add to df\n",
    "train_tokens = merged_train.apply(tokenize_post, axis = 1)\n",
    "train_tokens_df = pd.DataFrame(train_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "train_tokenized_df = pd.concat([merged_train.reset_index(drop=True), train_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(train_tokenized_df.columns.tolist())\n",
    "\n",
    "# Tokenize and add to df\n",
    "dev_tokens = merged_dev.apply(tokenize_post, axis = 1)\n",
    "dev_tokens_df = pd.DataFrame(dev_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "dev_tokenized_df = pd.concat([merged_dev.reset_index(drop=True), dev_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "# Tokenize and add to df\n",
    "test_tokens = merged_test.apply(tokenize_post, axis = 1)\n",
    "test_tokens_df = pd.DataFrame(test_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "test_tokenized_df = pd.concat([merged_test.reset_index(drop=True), test_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "train_tokenized_df.to_json(\"/Users/sabrina/Computational Social Science/final-project/data/tokenized/train_tokenized.json\", orient = \"records\", lines = True)\n",
    "dev_tokenized_df.to_json(\"/Users/sabrina/Computational Social Science/final-project/data/tokenized/dev_tokenized.json\", orient = \"records\", lines = True)\n",
    "test_tokenized_df.to_json(\"/Users/sabrina/Computational Social Science/final-project/data/tokenized/test_tokenized.json\", orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_string_to_list(string):\n",
    "\treturn ast.literal_eval(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Computational Social Science/final-project/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[100], line 18\u001b[0m\n    train_tokenized_df = pd.read_csv(\"/Users/sabrina/Computational Social Science/final-project/data/tokenized/train_tokenized.csv\", converters={\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Computational Social Science/final-project/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m in \u001b[1;35mread_csv\u001b[0m\n    return _read(filepath_or_buffer, kwds)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Computational Social Science/final-project/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m in \u001b[1;35m_read\u001b[0m\n    return parser.read(nrows)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Computational Social Science/final-project/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m in \u001b[1;35mread\u001b[0m\n    ) = self._engine.read(  # type: ignore[attr-defined]\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Computational Social Science/final-project/local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m in \u001b[1;35mread\u001b[0m\n    chunks = self._reader.read_low_memory(nrows)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32mparsers.pyx:838\u001b[0m in \u001b[1;35mpandas._libs.parsers.TextReader.read_low_memory\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32mparsers.pyx:921\u001b[0m in \u001b[1;35mpandas._libs.parsers.TextReader._read_rows\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32mparsers.pyx:1045\u001b[0m in \u001b[1;35mpandas._libs.parsers.TextReader._convert_column_data\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32mparsers.pyx:2116\u001b[0m in \u001b[1;35mpandas._libs.parsers._apply_converter\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[99], line 3\u001b[0m in \u001b[1;35mconvert_string_to_list\u001b[0m\n    return ast.literal_eval(string)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py:62\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py:50\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [[  101  3021 19031  3406  2140  1998  3841 21146 18098  3695  1010  2048\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# For loading data back in rather than rerunning above\n",
    "merged_train_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/merged/merged_train.csv\"\n",
    "merged_dev_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/merged/merged_dev.csv\"\n",
    "merged_test_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/merged/merged_test.csv\"\n",
    "\n",
    "merged_train = pd.read_csv(merged_train_savepath)\n",
    "merged_dev = pd.read_csv(merged_dev_savepath)\n",
    "merged_test = pd.read_csv(merged_test_savepath)\n",
    "\n",
    "merged_train_impl_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/with_impl/merged_train.csv\"\n",
    "merged_dev_impl_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/with_impl/merged_dev.csv\"\n",
    "merged_test_impl_savepath = \"/Users/sabrina/Computational Social Science/final-project/data/with_impl/merged_test.csv\"\n",
    "\n",
    "merged_train_impl = pd.read_csv(merged_train_impl_savepath)\n",
    "merged_dev_impl = pd.read_csv(merged_dev_impl_savepath)\n",
    "merged_test_impl = pd.read_csv(merged_test_impl_savepath)\n",
    "\n",
    "train_tokenized_df = pd.read_json(\"/Users/sabrina/Computational Social Science/final-project/data/tokenized/train_tokenized.csv\", orient = \"records\", lines = True)\n",
    "dev_tokenized_df = pd.read_json(\"/Users/sabrina/Computational Social Science/final-project/data/tokenized/dev_tokenized.csv\", orient = \"records\", lines = True)\n",
    "test_tokenized_df = pd.read_json(\"/Users/sabrina/Computational Social Science/final-project/data/tokenized/test_tokenized.csv\", orient = \"records\", lines = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BiasDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        print(\"test\")\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        token_type_ids = self.dataset.loc[idx, \"token_type_ids\"]\n",
    "        target_labels = self.dataset.loc[idx, \"target_labels\"]\n",
    "        \n",
    "        print(input_ids.shape)\n",
    "        print(type(attention_mask))\n",
    "        print(type(token_type_ids))\n",
    "        print(type(target_labels))\n",
    "        print(\"test2\")\n",
    "        # Create input tensors\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long).squeeze(dim=0),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long).squeeze(dim=0),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long).squeeze(dim=0),\n",
    "            \"target_labels\": torch.tensor(target_labels, dtype=torch.long).squeeze(dim=0)\n",
    "        }\n",
    "\n",
    "        print(\"test3\")\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    BiasDataset(train_tokenized_df),\n",
    "    batch_size=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    BiasDataset(test_tokenized_df),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RegenerativeTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(RegenerativeTransformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.embedding = nn.Embedding(30522, 768)\n",
    "        self.output = nn.Linear(768, 30522)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        memory = self.encoder(src, attention_mask=src_mask)[0]  # Ensure output matches expected format\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrina/Computational Social Science/final-project/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "def train_loop(dataloader, model, loss_fn, encoder_optimizer, decoder_optimizer, start_token_id):\n",
    "    train_loss = 0\n",
    "    # set the model to training model\n",
    "    model.train()\n",
    "    iter_count = 0\n",
    "    # for batch in dataloader:\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        # previous tokens\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "        \n",
    "        target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "        if target_pad > 0:\n",
    "             target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "        \n",
    "        # print(f\"Input ids: {input_ids.shape}\")\n",
    "        # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "        encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        encoder_state = encoder_outputs.last_hidden_state\n",
    "        # Decoder part initiated\n",
    "        \n",
    "        start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "        start_token_embed = model.embedding(start_token_id)\n",
    "        decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "        \n",
    "        # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            print(f\"Decoder input: {decoder_input.shape}\")\n",
    "            print(f\"Encoder output: {encoder_state.shape}\")\n",
    "            decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "            \n",
    "            logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "            outputs.append(logits)\n",
    "            \n",
    "            decoder_input = target_labels[0, i]\n",
    "            decoder_input = model.embedding(decoder_input).repeat(input_ids.size(0), 1, 1)\n",
    "\n",
    "            # _, topi = logits.topk(1)\n",
    "            # decoder_input = topi.squeeze().detach()\n",
    "            # decoder_input = model.embedding(decoder_input).repeat(input_ids.size(0), 1, 1)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "        print(f\"OUTPUTS: {outputs.shape}\")\n",
    "        print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "        \n",
    "        # preds = torch.argmax(outputs, dim = 2)\n",
    "        mask = target_labels != 0\n",
    "        loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "        print(f\"Loss shape: {loss.shape}\")\n",
    "        loss = loss.view(target_labels.shape)\n",
    "        masked_loss = loss * mask.float()\n",
    "\n",
    "        loss_sum = masked_loss.sum()\n",
    "        num_valid_tokens = mask.sum()\n",
    "        loss = loss_sum / num_valid_tokens.float()\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "        if iter_count % 5 == 0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        iter_count += 1\n",
    "    \n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, tokenizer, start_token_id):\n",
    "\n",
    "    all_sentences = []\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            # previous tokens\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "            \n",
    "            target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "            if target_pad > 0:\n",
    "                target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "            \n",
    "            # print(f\"Input ids: {input_ids.shape}\")\n",
    "            # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "            encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "            encoder_state = encoder_outputs.last_hidden_state\n",
    "            \n",
    "            start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "            start_token_embed = model.embedding(start_token_id)\n",
    "            decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "            \n",
    "            # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)  # Start token\n",
    "            outputs = []\n",
    "\n",
    "            for i in range(MAX_LENGTH):\n",
    "                print(f\"Decoder input: {decoder_input.shape}\")\n",
    "                print(f\"Encoder output: {encoder_state.shape}\")\n",
    "                decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "                \n",
    "                logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "                outputs.append(logits)\n",
    "\n",
    "                _, topi = logits.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = model.embedding(decoder_input).repeat(input_ids.size(0), 1, 1)\n",
    "\n",
    "            outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "            # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "            # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim = 2)\n",
    "            mask = target_labels != 0\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "            loss = loss.view(target_labels.shape)\n",
    "            masked_loss = loss * mask.float()\n",
    "\n",
    "            loss_sum = masked_loss.sum()\n",
    "            num_valid_tokens = mask.sum()\n",
    "            loss = loss_sum / num_valid_tokens.float()\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            decoded_sentences = [tokenizer.decode(pred, skip_special_tokens=True) for pred in preds]\n",
    "            all_sentences.extend(decoded_sentences)\n",
    "\n",
    "    return val_loss / len(dataloader), all_sentences\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrina/Computational Social Science/final-project/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer\n",
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "import torch.optim as optim\n",
    "\n",
    "encoder = FunnelModel.from_pretrained(\"funnel-transformer/small\")\n",
    "encoder_tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "print(encoder_tokenizer.vocab_size)\n",
    "decoder_layer = TransformerDecoderLayer(d_model=768, nhead=8, batch_first = True)\n",
    "decoder = TransformerDecoder(decoder_layer, num_layers = 6)\n",
    "\n",
    "regenerative_model = RegenerativeTransformer(encoder, decoder)\n",
    "\n",
    "encoder_optimizer = optim.Adam(regenerative_model.encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(regenerative_model.decoder.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(encoder_tokenizer.pad_token)\n",
    "print(encoder_tokenizer.convert_tokens_to_ids(encoder_tokenizer.pad_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35504 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "(1, 220)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "test2\n",
      "test3\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "Decoder input: torch.Size([1, 1, 768])\n",
      "Encoder output: torch.Size([1, 220, 768])\n",
      "OUTPUTS: torch.Size([1, 50, 30522])\n",
      "TARGET LABELS: torch.Size([1, 50])\n",
      "Loss shape: torch.Size([50])\n",
      "Loss: 9.643340110778809\n",
      "tensor(9.6433, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35504 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bos_token_id \u001b[38;5;241m=\u001b[39m encoder_tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(encoder_tokenizer\u001b[38;5;241m.\u001b[39mbos_token)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregenerative_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbos_token_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[174], line 74\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, encoder_optimizer, decoder_optimizer, start_token_id)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iter_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m---> 74\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     77\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Computational Social Science/final-project/local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computational Social Science/final-project/local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bos_token_id = encoder_tokenizer.convert_tokens_to_ids(encoder_tokenizer.bos_token)\n",
    "train_loop(train_dataloader, regenerative_model, loss_fn, encoder_optimizer, decoder_optimizer, bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "\ttrain_loss = train_loop(train_dataloader, regenerative_model, loss_fn, encoder_optimizer, decoder_optimizer, bos_token_id)\n",
    "\tprint(f\"Epoch: {train_loss}\")\n",
    "\ttrain_losses.append(train_loss)\n",
    "\tif epoch % 5 == 0:\n",
    "\t\tval_loss, val_sentences = test_loop(test_dataloader, regenerative_model, loss_fn, encoder_tokenizer, bos_token_id)\n",
    "\t\tval_losses.append(val_loss)\n",
    "\t\tcheckpoint = {\n",
    "\t\t\t\"model\": regenerative_model.state_dict(),\n",
    "\t\t\t\"encoder_optimizer\": encoder_optimizer.state_dict(),\n",
    "\t\t\t\"decoder_optimizer\": decoder_optimizer.state_dict(),\n",
    "\t\t\t\"train_losses\": train_losses,\n",
    "\t\t\t\"val_losses\": val_losses,\n",
    "\t\t\t\"val_sentences\": val_sentences,\n",
    "\t\t\t\"epoch\": epoch\n",
    "\t\t}\n",
    "\t\ttorch.save(checkpoint, f\"./checkpoints/checkpoint_{epoch}.pt\")\n",
    "\n",
    "\n",
    "print(f\"Train Losses: {train_losses}\")\n",
    "print(f\"Val Losses: {val_losses}\")\n",
    "\n",
    "for sent in val_sentences:\n",
    "\tprint(sent)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrina/Computational Social Science/final-project/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.1376, -0.3090,  0.6842,  ..., -0.4041,  0.8737, -0.7017],\n",
      "         [-0.1957, -0.2551,  0.0395,  ...,  0.2188,  0.1387,  0.1632],\n",
      "         [-0.2267,  0.3701,  0.0961,  ...,  0.0500, -0.0244, -0.1002],\n",
      "         ...,\n",
      "         [-0.5314,  0.7220,  0.2493,  ...,  0.0725,  0.0889, -0.0084],\n",
      "         [-0.1779,  0.1923,  0.5646,  ...,  0.4919,  0.7747, -1.1734],\n",
      "         [-0.1268,  0.0407,  0.0438,  ...,  0.6065,  1.0749, -1.3630]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "model = FunnelModel.from_pretrained(\"funnel-transformer/small\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
