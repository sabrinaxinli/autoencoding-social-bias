{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_save_path = \"./extracted/SBIC.v2.trn.csv\"\n",
    "dev_data_save_path = \"./extracted/SBIC.v2.dev.csv\"\n",
    "test_data_save_path = \"./extracted/SBIC.v2.tst.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while extracting the file: [Errno 2] No such file or directory: '/Users/sabrina/Computational Social Science/final-project/SBIC.v2.tgz'\n",
      "Error occurred while reading the file: [Errno 2] No such file or directory: '/Users/sabrina/Computational Social Science/final-project/extracted\\\\SBIC.v2.tst.csv'\n"
     ]
    }
   ],
   "source": [
    "from process_data import read_tgz_data\n",
    "\n",
    "train_data = read_tgz_data(train_data_save_path)\n",
    "dev_data = read_tgz_data(dev_data_save_path)\n",
    "test_data = read_tgz_data(test_data_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_context_str(category, target_group, target_stereotype):\n",
    "    category, target_group, target_stereotype = str(category), str(target_group), str(target_stereotype)\n",
    "\n",
    "    if target_stereotype.startswith(target_group):\n",
    "        return \" \".join((category, \":\", target_stereotype))\n",
    "    else:\n",
    "        return \" \".join((category, \":\", target_group, target_stereotype))\n",
    "\n",
    "def normalize_spacing(input):\n",
    "    cleaned_string = input.strip()\n",
    "    cleaned_string = ' '.join(cleaned_string.split())\n",
    "    return cleaned_string\n",
    "\n",
    "def remove_html_entities(text):\n",
    "    return re.sub(r\"&#[0-9]+;\", \"\", text)\n",
    "\n",
    "def remove_rt_username(text):\n",
    "    return re.sub(r\"RT @\\w+\\s*:\", \"\", text)\n",
    "\n",
    "def remove_beginning_and_ending_tags(text):\n",
    "    text = re.sub(r\"^(?:@[A-Za-z0-9_]+ )+\", \"\", text)\n",
    "    text = re.sub(r\"(?: @[A-Za-z0-9_]+)+$\", \"\", text)\n",
    "    text = re.sub(r\"^(?:@[A-Za-z0-9_]+[^\\w\\s]? )+\", \"\", text)\n",
    "    text = re.sub(r\"^(?:\\.\\s*)?(?:@[A-Za-z0-9_]+ )+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_start_html(text):\n",
    "    return re.sub(r\"&gt;\", \"\", text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "                               \"\\U0001F300-\\U0001F5FF\"\n",
    "                               \"\\U0001F680-\\U0001F6FF\"\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return re.sub(emoji_pattern, \"\", text)\n",
    "\n",
    "def clean_post(post):\n",
    "    post = remove_html_entities(post)\n",
    "    post = remove_rt_username(post)\n",
    "    post = remove_beginning_and_ending_tags(post)\n",
    "    post = remove_emojis(post)\n",
    "    post = remove_start_html(post)\n",
    "    post = normalize_spacing(post)\n",
    "    post = re.sub(r\"@\", \"\", post)\n",
    "    post = re.sub(r\"#\", \"\", post)\n",
    "    return post\n",
    "\n",
    "def merge_same_posts(posts):\n",
    "    first_row = posts.iloc[0]\n",
    "    posts = posts.fillna(\"nan\")\n",
    "    implications = posts[\"targetStereotype\"].tolist()\n",
    "    targeted_groups = posts[\"targetMinority\"].tolist()\n",
    "    targeted_categories = posts[\"targetCategory\"].tolist()\n",
    "    \n",
    "    contexts = list(zip(targeted_categories, targeted_groups, implications))\n",
    "    nan_context = (\"nan\", \"nan\", \"nan\")\n",
    "    filtered_contexts = [context for context in contexts if context != nan_context]\n",
    "    contexts = [get_context_str(tcat, tgroup, implication) for (tcat, tgroup, implication) in filtered_contexts]\n",
    "    # print(f\"Contexts: {contexts}\")\n",
    "    first_row[\"context\"] = contexts\n",
    "\n",
    "    raw_post = first_row[\"post\"]\n",
    "    first_row[\"post\"] = clean_post(raw_post)\n",
    "    return first_row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabri\\AppData\\Local\\Temp\\ipykernel_16520\\602313721.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_train = train_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
      "C:\\Users\\sabri\\AppData\\Local\\Temp\\ipykernel_16520\\602313721.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_dev = dev_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
      "C:\\Users\\sabri\\AppData\\Local\\Temp\\ipykernel_16520\\602313721.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  merged_test = test_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n"
     ]
    }
   ],
   "source": [
    "merged_train = train_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
    "merged_dev = dev_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)\n",
    "merged_test = test_data.groupby(\"post\").apply(merge_same_posts).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged\n",
    "merged_train_savepath = \"./data/merged/merged_train.csv\"\n",
    "merged_dev_savepath = \"./data/merged/merged_dev.csv\"\n",
    "merged_test_savepath = \"./data/merged/merged_test.csv\"\n",
    "\n",
    "merged_train.to_csv(merged_train_savepath, index=False)\n",
    "merged_dev.to_csv(merged_dev_savepath, index=False)\n",
    "merged_test.to_csv(merged_test_savepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_len(x):\n",
    "    return len(x) if isinstance(x, list) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with impl\n",
    "merged_train_impl_savepath = \"./data/with_impl/merged_train.csv\"\n",
    "merged_dev_impl_savepath = \"./data/with_impl/merged_dev.csv\"\n",
    "merged_test_impl_savepath = \"./data/with_impl/merged_test.csv\"\n",
    "\n",
    "merged_train_impl = merged_train[merged_train[\"context\"].map(safe_len) > 0]\n",
    "merged_dev_impl = merged_dev[merged_dev[\"context\"].map(safe_len) > 0]\n",
    "merged_test_impl = merged_test[merged_test[\"context\"].map(safe_len) > 0]\n",
    "\n",
    "merged_train_impl.to_csv(merged_train_impl_savepath, index=False)\n",
    "merged_dev_impl.to_csv(merged_dev_impl_savepath, index=False)\n",
    "merged_test_impl.to_csv(merged_test_impl_savepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 1010, 2088,  999,  102,    0,    0],\n",
      "        [ 101, 2182, 2003, 1037, 3231, 5164,  999,  102]]), 'token_type_ids': tensor([[2, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [2, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Testing to see tokenizer outputs\n",
    "import torch\n",
    "from transformers import FunnelTokenizer\n",
    "\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "\n",
    "# Tests\n",
    "texts = [\"Hello, world!\", \"Here is a test string!\"]\n",
    "\n",
    "tokenized_output = tokenizer.batch_encode_plus(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "model = FunnelModel.from_pretrained(\"funnel-transformer/small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "MAX_TOK_LEN = 70\n",
    "# Tokenize post helper function\n",
    "def tokenize_post(row):\n",
    "    context_str = \" <sep> \".join(row[\"context\"]) + \" <sep>\"\n",
    "    post_str = \"<cls> \" + row[\"post\"]\n",
    "    input_str = post_str + context_str\n",
    "    \n",
    "    post_data = row[\"post\"]\n",
    "    # print(f\"Input str: {input_str}\")\n",
    "    tokenized_output = tokenizer(input_str, padding=\"max_length\", add_special_tokens=False, truncation=True, max_length = MAX_TOK_LEN, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_output[\"input_ids\"].numpy()\n",
    "    attention_mask = tokenized_output[\"attention_mask\"].numpy()\n",
    "    token_type_ids = tokenized_output[\"token_type_ids\"].numpy()\n",
    "    \n",
    "    tokenized_post = tokenizer(post_data, padding=\"max_length\", max_length = MAX_TOK_LEN, truncation=True, return_tensors=\"pt\")\n",
    "    target_labels = tokenized_post[\"input_ids\"].numpy()\n",
    "    # token_type_ids_np = np.zeros_like(tokenized_output[\"token_type_ids\"])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"target_labels\": target_labels\n",
    "    }\n",
    "\n",
    "def tokenize_post_no_context(row):\n",
    "    post_str = \"<cls> \" + row[\"post\"] + \" <sep>\"\n",
    "    input_str = post_str\n",
    "    \n",
    "    post_data = row[\"post\"]\n",
    "    # print(f\"Input str: {input_str}\")\n",
    "    tokenized_output = tokenizer(input_str, padding=\"max_length\", add_special_tokens=False, truncation=True, max_length = MAX_TOK_LEN, return_tensors=\"pt\")\n",
    "    \n",
    "    input_ids = tokenized_output[\"input_ids\"].numpy()\n",
    "    attention_mask = tokenized_output[\"attention_mask\"].numpy()\n",
    "    token_type_ids = tokenized_output[\"token_type_ids\"].numpy()\n",
    "    \n",
    "    tokenized_post = tokenizer(post_data, padding=\"max_length\", max_length = MAX_TOK_LEN, truncation=True, return_tensors=\"pt\")\n",
    "    target_labels = tokenized_post[\"input_ids\"].numpy()\n",
    "    # token_type_ids_np = np.zeros_like(tokenized_output[\"token_type_ids\"])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"target_labels\": target_labels\n",
    "    }\n",
    "\n",
    "def t5_tokenize_post_reconstruction_prompt(row):\n",
    "    summary = row[\"summary\"]\n",
    "    context = row[\"context\"]\n",
    "    input_prompt = f\"Based on this summary: <{summary}>, and the implications of the post <{context}>, reconstruct the post.\"\n",
    "    tokenized_output = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    return tokenized_output.numpy()\n",
    "\n",
    "# Contexts is a list of strings\n",
    "# def tokenize_context(contexts):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "#     for context in contexts:\n",
    "#         tokenized_output = tokenizer(context, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         context_tokens.append((tokenized_output[\"input_ids\"], tokenized_output[\"token_type_ids\"], tokenized_output[\"attention_mask\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whoTarget', 'intentYN', 'sexYN', 'sexReason', 'offensiveYN', 'annotatorGender', 'annotatorMinority', 'sexPhrase', 'speakerMinorityYN', 'WorkerId', 'HITId', 'annotatorPolitics', 'annotatorRace', 'annotatorAge', 'post', 'targetMinority', 'targetCategory', 'targetStereotype', 'dataSource', 'context', 'input_ids', 'token_type_ids', 'attention_mask', 'target_labels']\n"
     ]
    }
   ],
   "source": [
    "# ## FOR FUNNEL TRANSFORMER ###\n",
    "\n",
    "# # Tokenize and add to df\n",
    "# train_tokens = merged_train.apply(tokenize_post, axis = 1)\n",
    "# train_tokens_df = pd.DataFrame(train_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "# train_tokenized_df = pd.concat([merged_train.reset_index(drop=True), train_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# print(train_tokenized_df.columns.tolist())\n",
    "\n",
    "# # Tokenize and add to df\n",
    "# dev_tokens = merged_dev.apply(tokenize_post, axis = 1)\n",
    "# dev_tokens_df = pd.DataFrame(dev_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "# dev_tokenized_df = pd.concat([merged_dev.reset_index(drop=True), dev_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "# # Tokenize and add to df\n",
    "# test_tokens = merged_test.apply(tokenize_post, axis = 1)\n",
    "# test_tokens_df = pd.DataFrame(test_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "# test_tokenized_df = pd.concat([merged_test.reset_index(drop=True), test_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "# train_tokenized_df.to_json(\"./data/tokenized/train_tokenized.json\", orient = \"records\", lines = True)\n",
    "# dev_tokenized_df.to_json(\"./data/tokenized/dev_tokenized.json\", orient = \"records\", lines = True)\n",
    "# test_tokenized_df.to_json(\"./data/tokenized/test_tokenized.json\", orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whoTarget', 'intentYN', 'sexYN', 'sexReason', 'offensiveYN', 'annotatorGender', 'annotatorMinority', 'sexPhrase', 'speakerMinorityYN', 'WorkerId', 'HITId', 'annotatorPolitics', 'annotatorRace', 'annotatorAge', 'post', 'targetMinority', 'targetCategory', 'targetStereotype', 'dataSource', 'context', 'input_ids', 'token_type_ids', 'attention_mask', 'target_labels']\n"
     ]
    }
   ],
   "source": [
    "## FOR FUNNEL TRANSFORMER ### ### FOR WITH_IMPL\n",
    "\n",
    "# Tokenize and add to df\n",
    "train_tokens = merged_train_impl.apply(tokenize_post, axis = 1)\n",
    "train_tokens_df = pd.DataFrame(train_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "train_tokenized_df = pd.concat([merged_train_impl.reset_index(drop=True), train_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(train_tokenized_df.columns.tolist())\n",
    "\n",
    "# Tokenize and add to df\n",
    "dev_tokens = merged_dev_impl.apply(tokenize_post, axis = 1)\n",
    "dev_tokens_df = pd.DataFrame(dev_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "dev_tokenized_df = pd.concat([merged_dev_impl.reset_index(drop=True), dev_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "# Tokenize and add to df\n",
    "test_tokens = merged_test_impl.apply(tokenize_post, axis = 1)\n",
    "test_tokens_df = pd.DataFrame(test_tokens.tolist(), columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"target_labels\"])\n",
    "test_tokenized_df = pd.concat([merged_test_impl.reset_index(drop=True), test_tokens_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "train_tokenized_df.to_json(\"./data/tokenized/train_tokenized.json\", orient = \"records\", lines = True)\n",
    "dev_tokenized_df.to_json(\"./data/tokenized/dev_tokenized.json\", orient = \"records\", lines = True)\n",
    "test_tokenized_df.to_json(\"./data/tokenized/test_tokenized.json\", orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_string_to_list(string):\n",
    "\treturn ast.literal_eval(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# For loading data back in rather than rerunning above\n",
    "merged_train_savepath = \"./data/merged/merged_train.csv\"\n",
    "merged_dev_savepath = \"./data/merged/merged_dev.csv\"\n",
    "merged_test_savepath = \"./data/merged/merged_test.csv\"\n",
    "\n",
    "merged_train = pd.read_csv(merged_train_savepath)\n",
    "merged_dev = pd.read_csv(merged_dev_savepath)\n",
    "merged_test = pd.read_csv(merged_test_savepath)\n",
    "\n",
    "merged_train_impl_savepath = \"./data/with_impl/merged_train.csv\"\n",
    "merged_dev_impl_savepath = \"./data/with_impl/merged_dev.csv\"\n",
    "merged_test_impl_savepath = \"./data/with_impl/merged_test.csv\"\n",
    "\n",
    "merged_train_impl = pd.read_csv(merged_train_impl_savepath)\n",
    "merged_dev_impl = pd.read_csv(merged_dev_impl_savepath)\n",
    "merged_test_impl = pd.read_csv(merged_test_impl_savepath)\n",
    "\n",
    "train_tokenized_df = pd.read_json(\"./data/tokenized/train_tokenized.json\", orient = \"records\", lines = True)\n",
    "dev_tokenized_df = pd.read_json(\"./data/tokenized/dev_tokenized.json\", orient = \"records\", lines = True)\n",
    "test_tokenized_df = pd.read_json(\"./data/tokenized/test_tokenized.json\", orient = \"records\", lines = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BiasDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"test\")\n",
    "        input_ids = self.dataset.loc[idx, \"input_ids\"]\n",
    "        # print(f\"Input ids: {input_ids}\")\n",
    "        attention_mask = self.dataset.loc[idx, \"attention_mask\"]\n",
    "        token_type_ids = self.dataset.loc[idx, \"token_type_ids\"]\n",
    "        target_labels = self.dataset.loc[idx, \"target_labels\"]\n",
    "        \n",
    "        # print(type(input_ids))\n",
    "        # print(type(attention_mask))\n",
    "        # print(type(token_type_ids))\n",
    "        # print(type(target_labels))\n",
    "        # print(\"test2\")\n",
    "        # Create input tensors\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long).squeeze(dim=0),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long).squeeze(dim=0),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long).squeeze(dim=0),\n",
    "            \"target_labels\": torch.tensor(target_labels, dtype=torch.long).squeeze(dim=0)\n",
    "        }\n",
    "\n",
    "        # print(\"test3\")\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    BiasDataset(train_tokenized_df),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    BiasDataset(test_tokenized_df.iloc[:50]),\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RegenerativeTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(RegenerativeTransformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.embedding = nn.Embedding(30522, 768)\n",
    "        self.output = nn.Linear(768, 30522)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        memory = self.encoder(src, attention_mask=src_mask)[0]  # Ensure output matches expected format\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrina/Computational Social Science/final-project/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# MAX_LENGTH = 50\n",
    "# def train_loop(dataloader, model, loss_fn, encoder_optimizer, decoder_optimizer, start_token_id):\n",
    "#     train_loss = 0\n",
    "#     # set the model to training model\n",
    "#     model.train()\n",
    "#     iter_count = 0\n",
    "#     # for batch in dataloader:\n",
    "#     for batch in tqdm.tqdm(dataloader):\n",
    "#         encoder_optimizer.zero_grad()\n",
    "#         decoder_optimizer.zero_grad()\n",
    "        \n",
    "#         # previous tokens\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "#         target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "        \n",
    "#         target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "#         if target_pad > 0:\n",
    "#              target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "        \n",
    "#         # print(f\"Input ids: {input_ids.shape}\")\n",
    "#         # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "#         encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "#         encoder_state = encoder_outputs.last_hidden_state\n",
    "#         # Decoder part initiated\n",
    "        \n",
    "#         start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "#         start_token_embed = model.embedding(start_token_id)\n",
    "#         decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "        \n",
    "#         # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)\n",
    "#         outputs = []\n",
    "\n",
    "#         for i in range(MAX_LENGTH):\n",
    "#             # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "#             # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "#             decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "            \n",
    "#             logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "#             outputs.append(logits)\n",
    "            \n",
    "#             decoder_input = target_labels[:, i]\n",
    "#             decoder_input = model.embedding(decoder_input).unsqueeze(dim = 1)\n",
    "            \n",
    "\n",
    "#             # _, topi = logits.topk(1)\n",
    "#             # decoder_input = topi.squeeze().detach()\n",
    "#             # decoder_input = model.embedding(decoder_input).repeat(input_ids.size(0), 1, 1)\n",
    "\n",
    "#         outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "#         # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "#         # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "        \n",
    "#         # preds = torch.argmax(outputs, dim = 2)\n",
    "#         mask = target_labels != 0\n",
    "#         # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#         # # print(f\"Loss shape: {loss.shape}\")\n",
    "#         # loss = loss.view(target_labels.shape)\n",
    "#         # masked_loss = loss * mask.float()\n",
    "\n",
    "#         # loss_sum = masked_loss.sum()\n",
    "#         # num_valid_tokens = mask.sum()\n",
    "#         # loss = loss_sum / num_valid_tokens.float()\n",
    "\n",
    "#         loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#         mask = target_labels != 0\n",
    "#         masked_loss = loss * mask.view(-1).float()\n",
    "#         loss = masked_loss.sum() / mask.sum()\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "\n",
    "#         if iter_count % 5 == 0:\n",
    "#             print(f\"Loss: {loss.item()}\")\n",
    "#         loss.backward()\n",
    "\n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "\n",
    "#         iter_count += 1\n",
    "    \n",
    "#     return train_loss / len(dataloader)\n",
    "\n",
    "# def test_loop(dataloader, model, loss_fn, tokenizer, start_token_id):\n",
    "\n",
    "#     all_sentences = []\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm.tqdm(dataloader):\n",
    "#             # previous tokens\n",
    "#             input_ids = batch[\"input_ids\"].to(device)\n",
    "#             attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#             token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "#             target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "            \n",
    "#             target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "#             if target_pad > 0:\n",
    "#                 target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "            \n",
    "#             # print(f\"Input ids: {input_ids.shape}\")\n",
    "#             # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "#             encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "#             encoder_state = encoder_outputs.last_hidden_state\n",
    "            \n",
    "#             start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "#             start_token_embed = model.embedding(start_token_id)\n",
    "#             decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "            \n",
    "#             # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)  # Start token\n",
    "#             outputs = []\n",
    "\n",
    "#             for i in range(MAX_LENGTH):\n",
    "#                 print(f\"Decoder input: {decoder_input.shape}\")\n",
    "#                 # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "#                 decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "                \n",
    "#                 logits = model.output(decoder_output.squeeze(dim = 1))\n",
    "#                 outputs.append(logits)\n",
    "\n",
    "#                 _, topi = logits.topk(1)\n",
    "#                 decoder_input = topi.squeeze().detach()\n",
    "#                 decoder_input = model.embedding(decoder_input).unsqueeze(dim = 0).unsqueeze(dim = 0)\n",
    "\n",
    "#             outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "#             # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "#             # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "            \n",
    "#             preds = torch.argmax(outputs, dim = 2)\n",
    "#             # mask = target_labels != 0\n",
    "#             # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#             # loss = loss.view(target_labels.shape)\n",
    "#             # masked_loss = loss * mask.float()\n",
    "\n",
    "#             loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "#             mask = target_labels != 0\n",
    "#             masked_loss = loss * mask.view(-1).float()\n",
    "#             loss = masked_loss.sum() / mask.sum()\n",
    "\n",
    "#             loss_sum = masked_loss.sum()\n",
    "#             num_valid_tokens = mask.sum()\n",
    "#             loss = loss_sum / num_valid_tokens.float()\n",
    "#             print(f\"Loss: {loss.item()}\")\n",
    "#             val_loss += loss.item()\n",
    "            \n",
    "#             decoded_sentences = [tokenizer.decode(pred, skip_special_tokens=True) for pred in preds]\n",
    "#             all_sentences.extend(decoded_sentences)\n",
    "\n",
    "#     return val_loss / len(dataloader), all_sentences\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "def train_loop(dataloader, model, loss_fn, encoder_optimizer, decoder_optimizer, start_token_id):\n",
    "    train_loss = 0\n",
    "    # set the model to training model\n",
    "    model.train()\n",
    "    iter_count = 0\n",
    "    # for batch in dataloader:\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        # previous tokens\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "        \n",
    "        target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "        if target_pad > 0:\n",
    "             target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "        \n",
    "        # print(f\"Input ids: {input_ids.shape}\")\n",
    "        # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "        encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        encoder_state = encoder_outputs.last_hidden_state\n",
    "        # Decoder part initiated\n",
    "        \n",
    "        start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "        start_token_embed = model.embedding(start_token_id)\n",
    "        decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "        # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "        # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)\n",
    "        outputs = []\n",
    "\n",
    "        teacher_forcing_ratio = 0.5\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "            # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "            decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "            \n",
    "            logits = model.output(decoder_output)[:, -1, :]\n",
    "            outputs.append(logits)\n",
    "            # print(f\"Logits: {logits.shape}\")\n",
    "\n",
    "            _, topi = logits.topk(1)\n",
    "            \n",
    "            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "            if use_teacher_forcing and i < target_labels.size(1) - 1:\n",
    "                next_token = target_labels[:, i + 1]\n",
    "                # print(f\"Teacher forced: {next_token.shape}\")\n",
    "            else:\n",
    "                next_token = topi.squeeze().detach()\n",
    "                # print(f\"Autoregresesive: {next_token.shape}\")\n",
    "\n",
    "            # print(\"next tok\")\n",
    "            # print(next_token.shape)\n",
    "            # print(f\"I: {i}\")\n",
    "            next_input = model.embedding(next_token).unsqueeze(dim = 1)\n",
    "            # print(f\"Next input: {next_input.shape}\")\n",
    "            \n",
    "            decoder_input = torch.cat((decoder_input, next_input), dim=1)\n",
    "            # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "\n",
    "            # _, topi = logits.topk(1)\n",
    "            # decoder_input = topi.squeeze().detach()\n",
    "            # decoder_input = model.embedding(decoder_input).repeat(input_ids.size(0), 1, 1)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "        # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "        # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "        \n",
    "        # preds = torch.argmax(outputs, dim = 2)\n",
    "        mask = target_labels != 0\n",
    "        # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "        # # print(f\"Loss shape: {loss.shape}\")\n",
    "        # loss = loss.view(target_labels.shape)\n",
    "        # masked_loss = loss * mask.float()\n",
    "\n",
    "        # loss_sum = masked_loss.sum()\n",
    "        # num_valid_tokens = mask.sum()\n",
    "        # loss = loss_sum / num_valid_tokens.float()\n",
    "\n",
    "        loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "        mask = target_labels != 0\n",
    "        masked_loss = loss * mask.view(-1).float()\n",
    "        loss = masked_loss.sum() / mask.sum()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "        if iter_count % 5 == 0:\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        iter_count += 1\n",
    "    \n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, tokenizer, start_token_id):\n",
    "\n",
    "    all_sentences = []\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            # previous tokens\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            target_labels = batch[\"target_labels\"][:, :MAX_LENGTH].to(device)\n",
    "            \n",
    "            target_pad = MAX_LENGTH - target_labels.size(1)\n",
    "            if target_pad > 0:\n",
    "                target_labels = F.pad(target_labels, (0, target_pad), \"constant\", 0)\n",
    "            \n",
    "            # print(f\"Input ids: {input_ids.shape}\")\n",
    "            # print(f\"Attention_mask: {attention_mask.shape}\")\n",
    "            encoder_outputs = model.encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "            encoder_state = encoder_outputs.last_hidden_state\n",
    "            \n",
    "            start_token_id = torch.tensor([start_token_id], dtype=torch.long, device=device)\n",
    "            start_token_embed = model.embedding(start_token_id)\n",
    "            decoder_input = start_token_embed.repeat(input_ids.size(0), 1, 1)\n",
    "            \n",
    "            # decoder_input = torch.tensor([model.embedding(start_token_id)]*input_ids.size(0), device=device)  # Start token\n",
    "            outputs = []\n",
    "\n",
    "            for i in range(MAX_LENGTH):\n",
    "                # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "                # print(f\"Encoder output: {encoder_state.shape}\")\n",
    "                decoder_output = model.decoder(decoder_input, encoder_state)\n",
    "               \n",
    "                logits = model.output(decoder_output)[:, -1, :]\n",
    "                \n",
    "                outputs.append(logits)\n",
    "\n",
    "                _, topi = logits.topk(1)\n",
    "                next_token = topi.squeeze().detach()\n",
    "                # print(f\"First next token: {next_token.shape}\")\n",
    "                next_token = model.embedding(next_token).unsqueeze(dim = 0).unsqueeze(dim = 0)\n",
    "                # print(f\"Next token: {next_token.shape}\")\n",
    "                # print(f\"Decoder input {decoder_input.shape}\")\n",
    "                decoder_input = torch.cat((decoder_input, next_token), dim = 1)\n",
    "\n",
    "            outputs = torch.stack(outputs, dim=1)  # [batch_size, MAX_LENGTH, vocab_size]\n",
    "            # print(f\"OUTPUTS: {outputs.shape}\")\n",
    "            # print(f\"TARGET LABELS: {target_labels.shape}\")\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim = 2)\n",
    "            # mask = target_labels != 0\n",
    "            # loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "            # loss = loss.view(target_labels.shape)\n",
    "            # masked_loss = loss * mask.float()\n",
    "\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), target_labels.view(-1))\n",
    "            mask = target_labels != 0\n",
    "            masked_loss = loss * mask.view(-1).float()\n",
    "            loss = masked_loss.sum() / mask.sum()\n",
    "\n",
    "            loss_sum = masked_loss.sum()\n",
    "            num_valid_tokens = mask.sum()\n",
    "            loss = loss_sum / num_valid_tokens.float()\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            val_loss += loss.item()\n",
    "           \n",
    "            decoded_sentences = [tokenizer.decode(pred, skip_special_tokens=True) for pred in preds]\n",
    "            all_sentences.extend(decoded_sentences)\n",
    "\n",
    "    return val_loss / len(dataloader), all_sentences\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Transformer, TransformerDecoder, TransformerDecoderLayer\n",
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "import torch.optim as optim\n",
    "\n",
    "encoder = FunnelModel.from_pretrained(\"funnel-transformer/small\")\n",
    "encoder_tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "print(encoder_tokenizer.vocab_size)\n",
    "decoder_layer = TransformerDecoderLayer(d_model=768, nhead=8, batch_first = True)\n",
    "decoder = TransformerDecoder(decoder_layer, num_layers = 6)\n",
    "\n",
    "regenerative_model = RegenerativeTransformer(encoder, decoder).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(regenerative_model.encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(regenerative_model.decoder.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "0\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "print(encoder_tokenizer.pad_token)\n",
    "print(encoder_tokenizer.convert_tokens_to_ids(encoder_tokenizer.pad_token))\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = encoder_tokenizer.convert_tokens_to_ids(encoder_tokenizer.bos_token)\n",
    "# train_loop(train_dataloader, regenerative_model, loss_fn, encoder_optimizer, decoder_optimizer, bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input: torch.Size([1, 1, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 1, 768])\n",
      "Decoder input: torch.Size([1, 2, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 2, 768])\n",
      "Decoder input: torch.Size([1, 3, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 3, 768])\n",
      "Decoder input: torch.Size([1, 4, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 4, 768])\n",
      "Decoder input: torch.Size([1, 5, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 5, 768])\n",
      "Decoder input: torch.Size([1, 6, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 6, 768])\n",
      "Decoder input: torch.Size([1, 7, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 7, 768])\n",
      "Decoder input: torch.Size([1, 8, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 8, 768])\n",
      "Decoder input: torch.Size([1, 9, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 9, 768])\n",
      "Decoder input: torch.Size([1, 10, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 10, 768])\n",
      "Decoder input: torch.Size([1, 11, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 11, 768])\n",
      "Decoder input: torch.Size([1, 12, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 12, 768])\n",
      "Decoder input: torch.Size([1, 13, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 13, 768])\n",
      "Decoder input: torch.Size([1, 14, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 14, 768])\n",
      "Decoder input: torch.Size([1, 15, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 15, 768])\n",
      "Decoder input: torch.Size([1, 16, 768])\n",
      "First next token: torch.Size([])\n",
      "Next token: torch.Size([1, 1, 768])\n",
      "Decoder input torch.Size([1, 16, 768])\n",
      "Decoder input: torch.Size([1, 17, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregenerative_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbos_token_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 140\u001b[0m, in \u001b[0;36mtest_loop\u001b[1;34m(dataloader, model, loss_fn, tokenizer, start_token_id)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoder input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoder_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# print(f\"Encoder output: {encoder_state.shape}\")\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39moutput(decoder_output)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m    144\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(logits)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:494\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    491\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 494\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:892\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    890\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m    891\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m--> 892\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:2573\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2571\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2572\u001b[0m     )\n\u001b[1;32m-> 2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, regenerative_model, loss_fn, encoder_tokenizer, bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/380 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input: torch.Size([32, 1, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 0\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 2, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 1\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 3, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 2\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 4, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 3\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 5, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 4\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 6, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 5\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 7, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 6\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 8, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 7\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 9, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 8\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 10, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 9\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 11, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 10\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 12, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 11\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 13, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 12\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 14, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 13\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 15, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 14\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 16, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 15\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 17, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 16\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 18, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 17\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 19, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 18\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 20, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 19\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 21, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 20\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 22, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 21\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 23, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 22\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 24, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 23\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 25, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 24\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 26, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 25\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 27, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 26\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 28, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 27\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 29, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 28\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 30, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 29\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 31, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Teacher forced: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 30\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 32, 768])\n",
      "Logits: torch.Size([32, 30522])\n",
      "Autoregresesive: torch.Size([32])\n",
      "next tok\n",
      "torch.Size([32])\n",
      "I: 31\n",
      "Next input: torch.Size([32, 1, 768])\n",
      "Decoder input: torch.Size([32, 33, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/380 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> 7\u001b[0m \ttrain_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregenerative_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \ttrain_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[60], line 45\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, encoder_optimizer, decoder_optimizer, start_token_id)\u001b[0m\n\u001b[0;32m     40\u001b[0m teacher_forcing_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_LENGTH):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# print(f\"Decoder input: {decoder_input.shape}\")\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# print(f\"Encoder output: {encoder_state.shape}\")\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39moutput(decoder_output)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     48\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(logits)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:494\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    491\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 494\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:890\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    888\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 890\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    891\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[0;32m    892\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:904\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    898\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    899\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x,\n\u001b[0;32m    900\u001b[0m                        attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    901\u001b[0m                        key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[0;32m    902\u001b[0m                        is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    903\u001b[0m                        need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\autoencoding-social-bias\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "EPOCHS = 5\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "\ttrain_loss = train_loop(train_dataloader, regenerative_model, loss_fn, encoder_optimizer, decoder_optimizer, bos_token_id)\n",
    "\tprint(f\"Epoch: {train_loss}\")\n",
    "\ttrain_losses.append(train_loss)\n",
    "\tif epoch % 1 == 0:\n",
    "\t\tval_loss, val_sentences = test_loop(test_dataloader, regenerative_model, loss_fn, encoder_tokenizer, bos_token_id)\n",
    "\t\tval_losses.append(val_loss)\n",
    "\t\tcheckpoint = {\n",
    "\t\t\t\"model\": regenerative_model.state_dict(),\n",
    "\t\t\t\"encoder_optimizer\": encoder_optimizer.state_dict(),\n",
    "\t\t\t\"decoder_optimizer\": decoder_optimizer.state_dict(),\n",
    "\t\t\t\"train_losses\": train_losses,\n",
    "\t\t\t\"val_losses\": val_losses,\n",
    "\t\t\t\"val_sentences\": val_sentences,\n",
    "\t\t\t\"epoch\": epoch\n",
    "\t\t}\n",
    "\t\ttorch.save(checkpoint, f\"./checkpoints/checkpoint_{epoch}.pt\")\n",
    "\n",
    "\n",
    "print(f\"Train Losses: {train_losses}\")\n",
    "print(f\"Val Losses: {val_losses}\")\n",
    "\n",
    "for sent in val_sentences:\n",
    "\tprint(sent)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabrina/Computational Social Science/final-project/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.1376, -0.3090,  0.6842,  ..., -0.4041,  0.8737, -0.7017],\n",
      "         [-0.1957, -0.2551,  0.0395,  ...,  0.2188,  0.1387,  0.1632],\n",
      "         [-0.2267,  0.3701,  0.0961,  ...,  0.0500, -0.0244, -0.1002],\n",
      "         ...,\n",
      "         [-0.5314,  0.7220,  0.2493,  ...,  0.0725,  0.0889, -0.0084],\n",
      "         [-0.1779,  0.1923,  0.5646,  ...,  0.4919,  0.7747, -1.1734],\n",
      "         [-0.1268,  0.0407,  0.0438,  ...,  0.6065,  1.0749, -1.3630]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import FunnelTokenizer, FunnelModel\n",
    "tokenizer = FunnelTokenizer.from_pretrained(\"funnel-transformer/small\")\n",
    "model = FunnelModel.from_pretrained(\"funnel-transformer/small\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
